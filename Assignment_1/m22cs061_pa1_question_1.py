# -*- coding: utf-8 -*-
"""M22CS061_PA1_Question_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iAf6ddrf4VKBEDjC3YtLUR4JXFG-zYge

## Importing req libraries and load dataset and datloader
"""

#import require libraries
import torch
from torch import nn
import torchvision
from torchvision.datasets import CIFAR10
from torchvision.transforms import ToTensor
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
# from collections import OrderedDict

!pip install -q torchmetrics
from torchmetrics.classification import MulticlassAccuracy

# write device agnostic code
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

# get and set dataset
train_data = CIFAR10(root = 'data', train = True, 
                     transform = ToTensor(), download = True)

test_data = CIFAR10(root = 'data', train = False,
                    transform = ToTensor(), download = True)

len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)

train_data.data[0].shape

# fit dataset in 32 batch size dataloader
BATCH_SIZE = 32

train_dataloader = DataLoader(dataset = train_data, batch_size = BATCH_SIZE,
                              shuffle = True)

test_dataloader = DataLoader(dataset = test_data, batch_size = BATCH_SIZE,
                             shuffle = True)

print(f"total {len(train_dataloader)} train data loader of each {BATCH_SIZE} batch size")
print(f"total {len(test_dataloader)} test data loader of each {BATCH_SIZE} batch size")

# class name
class_names = train_data.classes
class_names

# give index to class
class_idx = train_data.class_to_idx
class_idx

"""## Visulize data"""

image, label = train_data[0]
print(f"image shape: {image.shape}")
plt.imshow(image.T.squeeze())
plt.title(class_names[label])

# look the size and shape of dataloader
train_feature_batch, train_label_batch = next(iter(train_dataloader))
train_feature_batch.shape, train_label_batch.shape

len(train_feature_batch)

# image from dataloader
torch.manual_seed(64)

random_idx = torch.randint(0, len(train_feature_batch), size = [1]).item()
image, label = train_feature_batch[random_idx], train_label_batch[random_idx]
plt.imshow(image.T.squeeze())
plt.title(class_names[label])

"""# building model"""

transforms = transforms.Grayscale()

# here we have 3 channel images so first we need to transforms the image
flatten = nn.Flatten()

# transforms = transforms.Grayscale()

x = transforms(train_feature_batch[0])

out_flatten = flatten(x)

print(f"Shape of x: {x.shape}")
print(f"Shape of output of flatten: {out_flatten.shape}")

"""### sigmoid as activation function"""

# build base model
class CIFARSigModelV1(nn.Module):
  def __init__(self, input_units: int, hidden_units: int, output_units: int, num_hidden_layer: int):
    super().__init__()

    layers = [nn.Flatten()]

    for i in range(num_hidden_layer):
      if i == 0:
        # layers[str(i)] = nn.Sigmoid(nn.Linear(in_features = input_units, out_features = hidden_units))
        print(f"--------------i: {i} -------------")
        layers.append(nn.Linear(in_features = input_units, out_features = hidden_units))
        layers.append(nn.Sigmoid())

      # elif i == num_hidden_layer:
      #   # layers[str(i)] = nn.Sigmoid(nn.Linear(in_features = hidden_units, out_features = output_units))
      #   layers.append(nn.Linear(in_features = hidden_units, out_features = output_units))
      #   layers.append(nn.Sigmoid())

      else:
        # layers[str(i)] = nn.Sigmoid(nn.Linear(in_features = hidden_units, out_features = hidden_units))
        print(f"--------------i: {i} -------------")
        layers.append(nn.Linear(in_features = hidden_units, out_features = hidden_units))
        layers.append(nn.Sigmoid())
      # i += 1
      # layers[str(i)] = nn.Sigmoid()
    layers.append(nn.Linear(in_features = hidden_units, out_features = output_units))
    layers.append(nn.Sigmoid())
    self.layer = nn.Sequential(*layers)# with single hidden layer

  def forward(self, x):
    return self.layer(x)

# sig_model_0 = CIFARSigModelV1(input_units = 1024, hidden_units = 10,
#                               output_units = len(class_names), num_hidden_layer = 1).to(device)
# sig_model_0

# initial_params = sig_model_0.state_dict()
# w1_old = initial_params['layer.1.weight'].detach().clone()
# w1_old

"""## Loss and Accuracy Function"""

# loss and accuracy
loss_fn = nn.CrossEntropyLoss()

accuracy_fn = MulticlassAccuracy(num_classes = len(class_names)).to(device)

"""## Train and Test Loop"""

# train loop
def train_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer,
               accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0

  for batch, (x_train, y_train) in enumerate(dataloader):

    if device == 'cuda':
      x_train, y_train = x_train.to(device), y_train.to(device) 

    # train
    model.train()

    # 1.Forward
    x_train = transforms(x_train)
    y_pred = model(x_train)

    # print(f"\ny_pred: {y_pred.shape} y_train:{y_train.shape}\n{torch.argmax(y_pred, dim = 1)}\n")
    # 2. Loss and accuracy
    loss = loss_fn(y_pred, y_train)
    train_loss += loss
    acc = accuracy_fn(y_train, torch.argmax(y_pred, dim = 1))
    train_acc += acc

    # 3. zero grad
    optimizer.zero_grad()

    # 4. backward
    loss.backward()

    # 5. optimizer step
    optimizer.step()

  train_loss /= len(dataloader)
  train_acc /= len(dataloader)

  print(f"Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}")

  return train_loss

# test loop
def test_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module, accuracy_fn, device: torch.device = device):
  
  test_loss, test_acc = 0, 0

  model.eval()
  with torch.inference_mode():
    for x_test, y_test in dataloader:

      if device == 'cuda':
        x_test, y_test = x_test.to(device), y_test.to(device) 

      x_test = transforms(x_test)

      # 1. Forward
      test_pred = model(x_test)

      # 2. Loss and Accuracy
      test_loss += loss_fn(test_pred, y_test)

      test_acc += accuracy_fn(y_test, torch.argmax(test_pred, dim = 1))

    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

  print(f"Test Loss: {test_loss:.4f} Test Accuracy: {test_acc:.4f}")

  return test_loss

"""## Build model with 1 hidden layer"""

sig_model_0 = CIFARSigModelV1(input_units = 1024, hidden_units = 10,
                              output_units = len(class_names), num_hidden_layer = 1).to(device)
print(f"Model Structure:\n{sig_model_0}\n")

optimizer = torch.optim.SGD(params = sig_model_0.parameters(), lr = 0.2) # optimizer function

initial_params = sig_model_0.state_dict()
w1_old_m0 = initial_params['layer.1.weight'].detach().clone()
print(f"First Layer initial weights:\n{w1_old_m0}\n")

# train model
epoches = 5

torch.manual_seed(64)
torch.cuda.manual_seed(64)

for epoch in tqdm(range(epoches)):

  print(f"Epoch: {epoch+1} --------------------------")

  train_loss = train_loop(model = sig_model_0, dataloader = train_dataloader,
                          loss_fn = loss_fn, optimizer = optimizer,
                          accuracy_fn = accuracy_fn, device = device)
  
  test_loss = test_loop(model = sig_model_0, dataloader = test_dataloader,
                        loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                        device = device)
  
  print("\n")

post_params = sig_model_0.state_dict()
w1_new_m0 = post_params['layer.1.weight'].detach().clone()
print(f"Fisrt Layer weight after model train:\n{w1_new_m0}\n")

print(f"Weight change:\n{(w1_new_m0) - (w1_old_m0)}\n")

# # train model
# from tqdm.auto import tqdm

# epoches = 5

# torch.manual_seed(64)
# torch.cuda.manual_seed(64)

# for epoch in tqdm(range(epoches)):

#   print(f"Epoch: {epoch+1} --------------------------")

#   train_loop(model = sig_model_0, dataloader = train_dataloader,
#              loss_fn = loss_fn, optimizer = optimizer,
#              accuracy_fn = accuracy_fn, device = device)
  
#   test_loop(model = sig_model_0, dataloader = test_dataloader,
#             loss_fn = loss_fn, accuracy_fn = accuracy_fn,
#             device = device)
  
#   print("\n")

# post_params = sig_model_0.state_dict()
# w1_new = post_params['layer.1.weight'].detach().clone()
# w1_new

# w1_old

# abs(w1_new) - abs(w1_old)

"""## Build model with 2 hidden layer"""

sig_model_1 = CIFARSigModelV1(input_units = 1024, hidden_units = 10,
                              output_units = len(class_names), num_hidden_layer = 2).to(device)
print(f"Model Structure:\n{sig_model_1}\n")

optimizer = torch.optim.SGD(params = sig_model_1.parameters(), lr = 0.2) # optimizer function

initial_params = sig_model_1.state_dict()
w1_old_m1 = initial_params['layer.1.weight'].detach().clone()
print(f"First Layer initial weights:\n{w1_old_m1}\n")

# train model
epoches = 5

torch.manual_seed(64)
torch.cuda.manual_seed(64)

for epoch in tqdm(range(epoches)):

  print(f"Epoch: {epoch+1} --------------------------")

  train_loss = train_loop(model = sig_model_1, dataloader = train_dataloader,
                          loss_fn = loss_fn, optimizer = optimizer,
                          accuracy_fn = accuracy_fn, device = device)
  
  tes_loss = test_loop(model = sig_model_1, dataloader = test_dataloader,
                       loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                       device = device)
  
  print("\n")

post_params = sig_model_1.state_dict()
w1_new_m1 = post_params['layer.1.weight'].detach().clone()
print(f"Fisrt Layer weight after model train:\n{w1_new_m1}\n")

print(f"Weight change:\n{(w1_new_m1) - (w1_old_m1)}\n")

"""## Build model with 3 hidden layer"""

sig_model_2 = CIFARSigModelV1(input_units = 1024, hidden_units = 10,
                              output_units = len(class_names), num_hidden_layer = 3).to(device)
print(f"Model Structure:\n{sig_model_2}\n")

optimizer = torch.optim.SGD(params = sig_model_2.parameters(), lr = 0.2) # optimizer function

initial_params = sig_model_2.state_dict()
w1_old_m2 = initial_params['layer.1.weight'].detach().clone()
print(f"First Layer initial weights:\n{w1_old_m2}\n")

# train model
epoches = 5

torch.manual_seed(64)
torch.cuda.manual_seed(64)

for epoch in tqdm(range(epoches)):

  print(f"Epoch: {epoch+1} --------------------------")

  train_loss = train_loop(model = sig_model_2, dataloader = train_dataloader,
                          loss_fn = loss_fn, optimizer = optimizer,
                          accuracy_fn = accuracy_fn, device = device)
  
  test_loss = test_loop(model = sig_model_2, dataloader = test_dataloader,
                        loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                        device = device)
  
  print("\n")

post_params = sig_model_2.state_dict()
w1_new_m2 = post_params['layer.1.weight'].detach().clone()
print(f"Fisrt Layer weight after model train:\n{w1_new_m2}\n")

print(f"Weight change:\n{(w1_new_m2) - (w1_old_m2)}\n")

"""## Build model with 4 hidden layer"""

sig_model_3 = CIFARSigModelV1(input_units = 1024, hidden_units = 10,
                              output_units = len(class_names), num_hidden_layer = 4).to(device)
print(f"Model Structure:\n{sig_model_3}\n")

optimizer = torch.optim.SGD(params = sig_model_3.parameters(), lr = 0.2) # optimizer function

initial_params = sig_model_3.state_dict()
w1_old_m3 = initial_params['layer.1.weight'].detach().clone()
print(f"First Layer initial weights:\n{w1_old_m3}\n")

# train model
epoches = 5

torch.manual_seed(64)
torch.cuda.manual_seed(64)

for epoch in tqdm(range(epoches)):

  print(f"Epoch: {epoch+1} --------------------------")

  train_loss = train_loop(model = sig_model_3, dataloader = train_dataloader,
                          loss_fn = loss_fn, optimizer = optimizer,
                          accuracy_fn = accuracy_fn, device = device)
  
  test_loss = test_loop(model = sig_model_3, dataloader = test_dataloader,
                        loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                        device = device)
  
  print("\n")

post_params = sig_model_3.state_dict()
w1_new_m3 = post_params['layer.1.weight'].detach().clone()
print(f"Fisrt Layer weight after model train:\n{w1_new_m3}\n")

print(f"Weight change:\n{(w1_new_m3) - (w1_old_m3)}\n")

"""# Building model as Tanh an Activation function"""

class CIFARTanhModel(nn.Module):
  def __init__(self, input_units: int, hidden_units: int, output_units: int, number_hidden_layers: int):
    super().__init__()

    layers = []

    for i in range(number_hidden_layers):
      if i == 0:
        layers.append(nn.Flatten())
        layers.append(nn.Linear(in_features = input_units, out_features = hidden_units))
        layers.append(nn.Tanh())

      else:
        layers.append(nn.Linear(in_features = hidden_units, out_features = hidden_units))
        layers.append(nn.Tanh())

    layers.append(nn.Linear(in_features = hidden_units, out_features = output_units))
    layers.append(nn.Tanh())

    self.layer = nn.Sequential(*layers)

  def forward(self, x):
    return self.layer(x)

"""Loss, Accuracy, train loop, test loop are already implemented before 
so no need to re create them simple we re use them

## Build model with 1 hidden layer
"""

# torch.manual_seed(64)
# tanh_model_1 = CIFARTanhModel(input_units = 1024, hidden_units = 10,
#                               output_units = len(class_names), number_hidden_layers = 1).to(device)
# print(f"Model Structue:\n{tanh_model_1}\n")


# optimizer = torch.optim.SGD(params = tanh_model_1.parameters(), lr = 0.1)   # optimizer

# initial_weight = tanh_model_1.state_dict()
# w1_old_tm1 = initial_weight['layer.1.weight'].detach().clone()
# print(f"Initial First layer weights:\n{w1_old_tm1}\n")

# # train model
# torch.manual_seed(64)
# epoches = 5

# for epoch in tqdm(range(epoches)):
#   print(f"epoch: {epoch+1} ------------------------")

#   train_loop(model = tanh_model_1, dataloader = train_dataloader,
#              loss_fn = loss_fn, optimizer = optimizer,
#              accuracy_fn = accuracy_fn, device = device)
  
#   test_loop(model = tanh_model_1, dataloader = test_dataloader,
#             loss_fn = loss_fn, accuracy_fn = accuracy_fn,
#             device= device)
  
#   print("\n")

# final_weight = tanh_model_1.state_dict()
# w1_new_tm1 = final_weight['layer.1.weight'].detach().clone()
# print(f"Final First layer weight:\n{w1_new_tm1}\n")

# print(f"Change in weight:\n{(w1_new_tm1) - (w1_old_tm1)}")

"""## Build model with 2 hidden layer"""

# torch.manual_seed(64)
# tanh_model_2 = CIFARTanhModel(input_units = 1024, hidden_units = 10,
#                               output_units = len(class_names), number_hidden_layers = 2).to(device)
# print(f"Model Structue:\n{tanh_model_2}\n")


# optimizer = torch.optim.SGD(params = tanh_model_2.parameters(), lr = 0.1)   # optimizer

# initial_weight = tanh_model_2.state_dict()
# w1_old_tm2 = initial_weight['layer.1.weight'].detach().clone()
# print(f"Initial First layer weights:\n{w1_old_tm2}\n")

# # train model
# torch.manual_seed(64)
# epoches = 5

# for epoch in tqdm(range(epoches)):
#   print(f"epoch: {epoch+1} ------------------------")

#   train_loop(model = tanh_model_2, dataloader = train_dataloader,
#              loss_fn = loss_fn, optimizer = optimizer,
#              accuracy_fn = accuracy_fn, device = device)
  
#   test_loop(model = tanh_model_2, dataloader = test_dataloader,
#             loss_fn = loss_fn, accuracy_fn = accuracy_fn,
#             device= device)
  
#   print("\n")

# final_weight = tanh_model_2.state_dict()
# w1_new_tm2 = final_weight['layer.1.weight'].detach().clone()
# print(f"Final First layer weight:\n{w1_new_tm2}\n")

# print(f"Change in weight:\n{(w1_new_tm2) - (w1_old_tm2)}")

"""## Build model with 3 hidden layer"""

# torch.manual_seed(64)
# tanh_model_3 = CIFARTanhModel(input_units = 1024, hidden_units = 10,
#                               output_units = len(class_names), number_hidden_layers = 3).to(device)
# print(f"Model Structue:\n{tanh_model_3}\n")


# optimizer = torch.optim.SGD(params = tanh_model_3.parameters(), lr = 0.1)   # optimizer

# initial_weight = tanh_model_3.state_dict()
# w1_old_tm3 = initial_weight['layer.1.weight'].detach().clone()
# print(f"Initial First layer weights:\n{w1_old_tm3}\n")

# # train model
# torch.manual_seed(64)
# epoches = 5

# for epoch in tqdm(range(epoches)):
#   print(f"epoch: {epoch+1} ------------------------")

#   train_loop(model = tanh_model_3, dataloader = train_dataloader,
#              loss_fn = loss_fn, optimizer = optimizer,
#              accuracy_fn = accuracy_fn, device = device)
  
#   test_loop(model = tanh_model_3, dataloader = test_dataloader,
#             loss_fn = loss_fn, accuracy_fn = accuracy_fn,
#             device= device)
  
#   print("\n")

# final_weight = tanh_model_3.state_dict()
# w1_new_tm3 = final_weight['layer.1.weight'].detach().clone()
# print(f"Final First layer weight:\n{w1_new_tm3}\n")

# print(f"Change in weight:\n{(w1_new_tm3) - (w1_old_tm3)}")

"""## Build model with 4 hidden layer"""

# torch.manual_seed(64)
# tanh_model_4 = CIFARTanhModel(input_units = 1024, hidden_units = 10,
#                               output_units = len(class_names), number_hidden_layers = 4).to(device)
# print(f"Model Structue:\n{tanh_model_4}\n")


# optimizer = torch.optim.SGD(params = tanh_model_4.parameters(), lr = 0.1)   # optimizer

# initial_weight = tanh_model_4.state_dict()
# w1_old_tm4 = initial_weight['layer.1.weight'].detach().clone()
# print(f"Initial First layer weights:\n{w1_old_tm4}\n")

# # train model
# torch.manual_seed(64)
# epoches = 5

# for epoch in tqdm(range(epoches)):
#   print(f"epoch: {epoch+1} ------------------------")

#   train_loop(model = tanh_model_4, dataloader = train_dataloader,
#              loss_fn = loss_fn, optimizer = optimizer,
#              accuracy_fn = accuracy_fn, device = device)
  
#   test_loop(model = tanh_model_4, dataloader = test_dataloader,
#             loss_fn = loss_fn, accuracy_fn = accuracy_fn,
#             device= device)
  
#   print("\n")

# final_weight = tanh_model_4.state_dict()
# w1_new_tm4 = final_weight['layer.1.weight'].detach().clone()
# print(f"Final First layer weight:\n{w1_new_tm4}\n")

# print(f"Change in weight:\n{(w1_new_tm4) - (w1_old_tm4)}")

"""## Build Model with differ ent hidden layer"""

torch.manual_seed(64)

for i in range(1, 11):
  print(f"\nModel with {i*2} hidden layer\n\n")
  tanh_model_1 = CIFARTanhModel(input_units = 1024, hidden_units = 10,
                                output_units = len(class_names), number_hidden_layers = i*2).to(device)
  print(f"Model Structue:\n{tanh_model_1}\n")


  optimizer = torch.optim.SGD(params = tanh_model_1.parameters(), lr = 0.2)   # optimizer

  initial_weight = tanh_model_1.state_dict()
  w1_old_tm1 = initial_weight['layer.1.weight'].detach().clone()
  print(f"Initial First layer weights:\n{w1_old_tm1}\n")

  # train model
  torch.manual_seed(64)
  epoches = 5

  prev_test_loss = 0
  for epoch in tqdm(range(epoches)):
    print(f"epoch: {epoch+1} ------------------------")

    train_loss = train_loop(model = tanh_model_1, dataloader = train_dataloader,
                            loss_fn = loss_fn, optimizer = optimizer,
                            accuracy_fn = accuracy_fn, device = device)
    
    if epoch != 0:
      prev_test_loss = test_loss

    test_loss = test_loop(model = tanh_model_1, dataloader = test_dataloader,
                          loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                          device= device)
    
    
    
    print("\n")

  final_weight = tanh_model_1.state_dict()
  w1_new_tm1 = final_weight['layer.1.weight'].detach().clone()
  print(f"Final First layer weight:\n{w1_new_tm1}\n")

  print(f"Change in weight:\n{(w1_new_tm1) - (w1_old_tm1)}")


  if abs(prev_test_loss - test_loss) <= 0.0005:
    break

"""## Methods to overcome the vanishing gradient problem

### use ReLu as activation function
"""

class CIFARReLUModel(nn.Module):
  def __init__(self, input_units: int, hidden_units: int, output_units: int, number_hidden_layers: int):
    super().__init__()

    layers = []

    for i in range(number_hidden_layers):
      if i == 0:
        layers.append(nn.Flatten())
        layers.append(nn.Linear(in_features = input_units, out_features = hidden_units))
        layers.append(nn.ReLU())

      else:
        layers.append(nn.Linear(in_features = hidden_units, out_features = hidden_units))
        layers.append(nn.ReLU())

    layers.append(nn.Linear(in_features = hidden_units, out_features = output_units))
    layers.append(nn.ReLU())

    self.layer = nn.Sequential(*layers)

  def forward(self, x):
    return self.layer(x)

torch.manual_seed(64)

for i in range(1, 11):
  print(f"\nModel with {i*2} hidden layer\n\n")
  relu_model_1 = CIFARReLUModel(input_units = 1024, hidden_units = 10,
                                output_units = len(class_names), number_hidden_layers = i*2).to(device)
  print(f"Model Structue:\n{relu_model_1}\n")


  optimizer = torch.optim.SGD(params = relu_model_1.parameters(), lr = 0.1)   # optimizer

  initial_weight = relu_model_1.state_dict()
  w1_old_tm1 = initial_weight['layer.1.weight'].detach().clone()
  print(f"Initial First layer weights:\n{w1_old_tm1}\n")

  # train model
  torch.manual_seed(64)
  epoches = 5

  prev_test_loss = 0
  for epoch in tqdm(range(epoches)):
    print(f"epoch: {epoch+1} ------------------------")

    train_loss = train_loop(model = relu_model_1, dataloader = train_dataloader,
                            loss_fn = loss_fn, optimizer = optimizer,
                            accuracy_fn = accuracy_fn, device = device)
    
    if epoch != 0:
      prev_test_loss = test_loss

    test_loss = test_loop(model = relu_model_1, dataloader = test_dataloader,
                          loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                          device= device)
    
    
    
    print("\n")

  final_weight = relu_model_1.state_dict()
  w1_new_tm1 = final_weight['layer.1.weight'].detach().clone()
  print(f"Final First layer weight:\n{w1_new_tm1}\n")

  print(f"Change in weight:\n{(w1_new_tm1) - (w1_old_tm1)}")


  if abs(prev_test_loss - test_loss) <= 0.0005:
    break

"""### Hyper parameter Turning
- like learning rate (make it small but not too small)

- Reduce network layer or reduce layer nodes
"""

