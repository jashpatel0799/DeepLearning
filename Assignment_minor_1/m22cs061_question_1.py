# -*- coding: utf-8 -*-
"""M22CS061_Question-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H08YWCM7XkKi15xby-gqe-RgEk2TzbCF

## Question 1
"""

# import libaraies
import torch
from torch import nn
import torchvision
from torchvision.datasets import CIFAR10
from torchvision.transforms import ToTensor
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

from torchvision.transforms.transforms import InterpolationMode

!pip install -q torchmetrics
from torchmetrics.classification import MulticlassAccuracy

# device agnostic code
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

# prepare augmentation
# Create training transform with RandomRotation and gaussion noise augmentation
train_transform_trivial_augment = transforms.Compose([
    transforms.RandomRotation(10, interpolation = InterpolationMode.NEAREST, expand = False),
    transforms.GaussianBlur(kernel_size = 3, sigma = 0.3),
    transforms.ToTensor() 
])

# Create training transform (no data augmentation)
train_transform = transforms.Compose([
    transforms.ToTensor()
])

# Create testing transform (no data augmentation)
test_transform = transforms.Compose([
    transforms.ToTensor()
])

# get datasets
train_data = CIFAR10(root = 'data', train = True, transform = train_transform, download = True) 
train_data_aug = CIFAR10(root = 'data', train = True, transform = train_transform_trivial_augment, download = True)
test_data = CIFAR10(root = 'data', train = False, transform = test_transform, download = True)
# test_data_aug = CIFAR10(root = 'data', train = False, transform = train_transform_trivial_augment, download = True)

# type(train_data)
train_data[0]

"""## Get only even class data"""

train_list = []
train_aug_list = []
test_list = []


for i, (x,y) in enumerate(train_data):
  if y % 2 == 0:
    train_list.append(train_data[i])
  # print(train_data[i])
  # break
for i, (x,y) in enumerate(train_data_aug):
  if y % 2 == 0:
    train_aug_list.append(train_data_aug[i])

for x,y in test_data:
  if y % 2 == 0:
    test_list.append((x, y))

len(train_list), len(train_aug_list), len(test_list)

# new_train = torch.utils.data.ConcatDataset(train_list)
# # type(new_train)
# len(test_list)
# test_list[:2]

# concate datasets
new_train_data = torch.utils.data.ConcatDataset((train_list, train_aug_list))
new_test_data = torch.utils.data.ConcatDataset([test_list])

# # # type(train_data)
# for i in new_test_data:
#   print(i)
#   break

len(new_test_data)

# for i in new_train_data:
#   print(i)
#   break

# # new_train_data[60000]
# for i, (x, y) in enumerate(new_train_data):
#   # print(x)
#   # print(type(y))
#   # break
#   if y % 2 != 0:
#     new_train_data.remove(i)

len(new_train_data), len(new_test_data)

# for i in new_test_data:
#   print(i)

# fit dataset in Dataloader
BATCH_SIZE = 32

train_dataloader = DataLoader(dataset = new_train_data, batch_size = BATCH_SIZE, shuffle = True)
test_dataloader = DataLoader(dataset = test_data, batch_size = BATCH_SIZE, shuffle = False)

print(f"total {len(train_dataloader)} train data loader of each {BATCH_SIZE} batch size")
print(f"total {len(test_dataloader)} test data loader of each {BATCH_SIZE} batch size")

for i in range(len(train_data.classes)):
  if i % 2 == 0:
    print(train_data.classes[i])
  # print(i)

# class name
class_names = [train_data.classes[i] for i in range(len(train_data.classes)) if i % 2 == 0] 
class_names



# give index to class
class_idx = train_data.class_to_idx
class_idx

"""## Visulize Data"""

image, label = train_data[0]
print(f"image shape: {image.shape}")
plt.imshow(image.T.squeeze())
plt.title(class_names[label//2])

# look the size and shape of dataloader
train_feature_batch, train_label_batch = next(iter(train_dataloader))
train_feature_batch.shape, train_label_batch.shape

# image from dataloader
torch.manual_seed(64)

random_idx = torch.randint(0, len(train_feature_batch), size = [1]).item()
image, label = train_feature_batch[random_idx], train_label_batch[random_idx]
plt.imshow(image.T.squeeze())
plt.title(class_names[label//2])

"""## Building model"""

from torch.nn.modules.activation import Tanh
# base modle
class CNNCIFAR10(nn.Module):
  def __init__(self, input_channel: int, output_channel: int, hidden_node: int, output_nodes: int):
    super().__init__()

    self.conv_block_1 = nn.Sequential(
        nn.Conv2d(in_channels = input_channel, out_channels = output_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh()
    )

    self.conv_block_2 = nn.Sequential(
        nn.Conv2d(in_channels = output_channel, out_channels = output_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh()
    )

    self.conv_block_3 = nn.Sequential(
        nn.Conv2d(in_channels = output_channel, out_channels = output_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh()
    )

    self.conv_block_4 = nn.Sequential(
        nn.Conv2d(in_channels = output_channel, out_channels = output_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh()
    )

    self.conv_block_5 = nn.Sequential(
        nn.Conv2d(in_channels = output_channel, out_channels = output_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh()
    )

    self.conv_block_6 = nn.Sequential(
        nn.Conv2d(in_channels = output_channel, out_channels = output_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 3, stride = 1)
    )

    self.fc1_layer = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features = output_channel*30*30, out_features = hidden_node),
        nn.Tanh(),
        nn.Linear(in_features = hidden_node, out_features = output_nodes)
    )

  def forward(self, x):
    conv_x =  self.conv_block_6(self.conv_block_5(self.conv_block_4(self.conv_block_3(self.conv_block_2(self.conv_block_1(x))))))
    # print(f"conv_x shape: {conv_x.shape}")
    return self.fc1_layer(conv_x)



base_model = CNNCIFAR10(input_channel = 3, output_channel = 12, hidden_node = 8, output_nodes = len(class_names)).to(device)
base_model

base_model.__class__.__name__

# base_model.__class__.__name__.find('BatchNorm'), base_model.__class__.__name__.find('Tanh')

"""## Xavier Initialization"""

# custom weights initialization called on netG and netD
# def weights_init(m):
#     classname = m.__class__.__name__
#     if classname.find('Conv') != -1:
#         torch.nn.init.normal_(m.weight, 0.0, 0.02)
#     elif classname.find('Linear') != -1:
#         torch.nn.init.normal_(m.weight, 1.0, 0.02)
#         torch.nn.init.zeros_(m.bias)
def weights_init(c_name):
    if isinstance(c_name, nn.Conv2d):
        torch.nn.init.xavier_uniform_(c_name.weight)
        torch.nn.init.zeros_(c_name.bias)
    elif isinstance(c_name, nn.Linear):
        torch.nn.init.xavier_uniform_(c_name.weight)
        torch.nn.init.zeros_(c_name.bias)

"""## Loss and Accuracy Fuction"""

# loss
loss_fn = nn.CrossEntropyLoss()

# accuracy_fn
accuracy_fn = MulticlassAccuracy(num_classes = len(class_names)).to(device)

"""## Train and Test loop"""

# train
def train_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, 
               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, 
               accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0

  for batch, (x_train, y_train) in enumerate(dataloader):

    if device == 'cuda':
      x_train, y_train = x_train.to(device), y_train.to(device)

    model.train()
    y_train = y_train // 2
    # 1. Forward
    y_pred = model(x_train)

    # 2. Loss
    loss = loss_fn(y_pred, y_train)
    acc = accuracy_fn(y_train, torch.argmax(y_pred, dim = 1))
    train_loss += loss
    train_acc += acc

    # 3. optimizer zero grad
    optimizer.zero_grad()

    # 4. Backward
    loss.backward()

    # 5. optimizer step
    optimizer.step()

  train_loss /= len(dataloader)
  train_acc /= len(dataloader)

  return train_loss, train_acc


def test_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module, accuracy_fn, device: torch.device = device):
  test_loss, test_acc = 0, 0

  model.eval()
  with torch.inference_mode():
    for x_test, y_test in dataloader:
      if device == 'cuda':
        x_test, y_test = x_test.to(device), y_test.to(device)

      y_test = y_test // 2
      # 1. Forward
      test_pred = model(x_test)

      # 2. Loss
      test_loss += loss_fn(test_pred, y_test)
      test_acc += accuracy_fn(y_test, torch.argmax(test_pred, dim = 1))

    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

  return test_loss, test_acc

"""## Loss and Accuracy plot vs Epoch"""

def plotplot(train_losses, test_losses, train_acces, test_acces):
  plt.figure(figsize = (25,8))
  # plt.plot(range(1, epoches*(len(train_dataloader)*BATCH_SIZE) + 1), train)
  plt.subplot(1,2,1)
  plt.plot(range(len(train_losses)),train_losses, label = "Train Loss")
  plt.plot(range(len(test_losses)),test_losses, label = "Test Loss")
  plt.xlabel("Epoches")
  plt.ylabel("Loss")
  plt.title("Loss vs Epoches")
  plt.legend()

  plt.subplot(1,2,2)
  plt.plot(range(len(train_acces)),train_acces, label = "Train Accuracy")
  plt.plot(range(len(test_acces)),test_acces, label = "Test Accuracy")
  plt.xlabel("Epoches")
  plt.ylabel("Accuracy")
  plt.title("Accuracy vs Epoches")
  plt.legend()

  plt.show()

torch.manual_seed(64)
torch.cuda.manual_seed(64)
cnn_cifar = CNNCIFAR10(input_channel = 3, output_channel = 12, hidden_node = 512, output_nodes = len(class_names)).to(device)

# initialize Xavier weights
cnn_cifar.apply(weights_init)

# optimizer function
optimizer = torch.optim.SGD(params = cnn_cifar.parameters(), lr = 0.01)


train_losses, test_losses = [], []
train_acces, test_acces = [], []

# train model
epoches = 10

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  train_loss, train_acc = train_loop(model = cnn_cifar, dataloader = train_dataloader,
                                     loss_fn = loss_fn, optimizer = optimizer, 
                                     accuracy_fn = accuracy_fn, device = device)
  test_loss, test_acc = test_loop(model = cnn_cifar, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  print(f"Epoch: {epoch + 1}  Train Loss: {train_loss:.4f} / Test Loss: {test_loss:.4f} -/- Train Accuracy: {train_acc:.4f} / Test Accuracy: {test_acc:.4f}")

  train_losses.append(train_loss.item())
  test_losses.append(test_loss.item())
  train_acces.append(train_acc.item())
  test_acces.append(test_acc.item())

plotplot(train_losses, test_losses, train_acces, test_acces)
# train_losses