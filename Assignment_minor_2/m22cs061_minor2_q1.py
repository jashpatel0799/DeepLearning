# -*- coding: utf-8 -*-
"""M22CS061_Minor2_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i1NLUrt1E98BlxGVAiB7IMS-9c63oLBf

## Import require libraries
"""

import torch
from torch import nn
import torchvision
from torchvision import models, datasets
from torchvision.transforms import ToTensor
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from torchvision.utils import make_grid
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from tqdm.auto import tqdm

import os
from random import randint
import urllib
import zipfile

!pip install -q torchmetrics
from torchmetrics.classification import MulticlassAccuracy

"""## Device Agnostic code"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

"""## Helper function"""

# Functions to display single or a batch of sample images
def imshow(img):
    npimg = img[0].numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.title(img[1])
    plt.show()
    
def show_batch(dataloader):
    dataiter = iter(dataloader)
    images, labels = dataiter.next()    
    imshow(make_grid(images)) # Using Torchvision.utils make_grid function
    
def show_image(dataloader):
    dataiter = iter(dataloader)
    images, labels = dataiter.next()
    random_num = randint(0, len(images)-1)
    imshow(images[random_num])
    label = labels[random_num]
    print(f'Label: {label}, Shape: {images[random_num].shape}')

"""## Get dataset"""

# Retrieve data directly from Stanford data source
!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
  
# Unzip raw zip file
!unzip -qq 'tiny-imagenet-200.zip'

# Define main data directory
DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]

# Define training and validation data paths
TRAIN_DIR = os.path.join(DATA_DIR, 'train') 
VALID_DIR = os.path.join(DATA_DIR, 'val')

"""## preprocess validation data"""

# Create separate validation subfolders for the validation images based on
# their labels indicated in the val_annotations txt file
val_img_dir = os.path.join(VALID_DIR, 'images')

# Open and read val annotations text file
fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')
data = fp.readlines()

# Create dictionary to store img filename (word 0) and corresponding
# label (word 1) for every line in the txt file (as key value pair)
val_img_dict = {}
for line in data:
    words = line.split('\t')
    val_img_dict[words[0]] = words[1]
fp.close()

# Display first 10 entries of resulting val_img_dict dictionary
{k: val_img_dict[k] for k in list(val_img_dict)[:10]}

# Create subfolders (if not present) for validation images based on label,
# and move images into the respective folders
for img, folder in val_img_dict.items():
    newpath = (os.path.join(val_img_dir, folder))
    if not os.path.exists(newpath):
        os.makedirs(newpath)
    if os.path.exists(os.path.join(val_img_dir, img)):
        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))

"""## Image preprocess transform"""

image_transform_pretrain = transforms.Compose([
                transforms.Resize(64), # Resize images to 64 x 64
                transforms.CenterCrop(56), # Center crop image
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),  # Images to tensors
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                     std=[0.229, 0.224, 0.225])
])

"""## make datasets"""

train_dataset = datasets.ImageFolder(TRAIN_DIR, transform = image_transform_pretrain)
valid_dataset = datasets.ImageFolder(val_img_dir, transform = image_transform_pretrain)

num_classes = len(train_dataset.classes)
num_classes

"""## Create Dataloader"""

# Define batch size for DataLoaders
BATCH_SIZE = 32

# Create DataLoaders for pre-trained models (normalized based on specific requirements)
train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, 
                              shuffle = True, drop_last = True)

valid_dataloader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, 
                              drop_last = True)

# for i, (x, y) in enumerate(train_dataloader):
#   print(x)
#   print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
#   print(y)
#   break

## visulize image
imshow(train_dataset[0])

"""# Build models

## Xavier Initialization
"""

def weights_init(c_name):
    if isinstance(c_name, nn.Conv2d):
        torch.nn.init.xavier_uniform_(c_name.weight)
        torch.nn.init.zeros_(c_name.bias)
    elif isinstance(c_name, nn.Linear):
        torch.nn.init.xavier_uniform_(c_name.weight)
        torch.nn.init.zeros_(c_name.bias)

"""## Teacher Model"""

class TeacherCNN(nn.Module):
  def __init__(self, in_channel: int, out_channel: int, hidden_channel:int,
               in_feature: int, out_feature: int):
    super().__init__()

    self.conv_block1 = nn.Sequential(
        nn.Conv2d(in_channels = in_channel, out_channels = out_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block2 = nn.Sequential(
        nn.Conv2d(in_channels = out_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block3 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block4 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block5 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block6 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block7 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block8 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block9 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block10 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block11 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
    )
    self.conv_block12 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 5, stride = 3)
    )
    self.fc1_layer = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features = hidden_channel*18*18, out_features = in_feature),
        nn.Tanh(),
        nn.Linear(in_features = in_feature, out_features = out_feature)
    )


  def forward(self, x):
    conv_x_half =  self.conv_block6(self.conv_block5(self.conv_block4(self.conv_block3(self.conv_block2(self.conv_block1(x))))))
    conv_x =  self.conv_block12(self.conv_block11(self.conv_block10(self.conv_block9(self.conv_block8(self.conv_block7(conv_x_half))))))
    # print(f"conv_x shape: {conv_x.shape}")
    return self.fc1_layer(conv_x)

teacher = TeacherCNN(in_channel = 3, out_channel = 12, hidden_channel = 10, in_feature = 512, out_feature = num_classes).to(device)
teacher

"""## Student Model"""

class StudentCNN(nn.Module):
  def __init__(self, in_channel: int, out_channel: int, hidden_channel:int,
               in_feature: int, out_feature: int):
    super().__init__()

    self.conv_block1 = nn.Sequential(
        nn.Conv2d(in_channels = in_channel, out_channels = out_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.ReLU(),
    )
    self.conv_block2 = nn.Sequential(
        nn.Conv2d(in_channels = out_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.ReLU(),
    )
    self.conv_block3 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.ReLU(),
    )
    self.conv_block4 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_channel, out_channels = hidden_channel, kernel_size = 5, stride = 1, padding = 2),
        nn.ReLU(),
        nn.AvgPool2d(kernel_size = 5, stride = 3)
    )

    self.fc1_layer = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features = hidden_channel*18*18, out_features = in_feature),
        nn.ReLU(),
        nn.Linear(in_features = in_feature, out_features = out_feature)
    )


  def forward(self, x):
    conv_x =  self.conv_block4(self.conv_block3(self.conv_block2(self.conv_block1(x))))
    # print(f"conv_x shape: {conv_x.shape}")
    return self.fc1_layer(conv_x)

student = StudentCNN(in_channel = 3, out_channel = 12, hidden_channel = 10, in_feature = 512, out_feature = num_classes).to(device)
student

"""## Loss and Accuray Functions"""

# Loss
loss_fn = nn.CrossEntropyLoss()

# Accuracy
accuracy_fn = MulticlassAccuracy(num_classes = num_classes).to(device)

"""## Train and Test Loop"""

# train
def train_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, 
               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, 
               accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0

  for batch, (x_train, y_train) in enumerate(dataloader):

    if device == 'cuda':
      x_train, y_train = x_train.to(device), y_train.to(device)

    model.train()
    # 1. Forward
    y_pred = model(x_train)

    # 2. Loss
    loss = loss_fn(y_pred, y_train)
    acc = accuracy_fn(y_train, torch.argmax(y_pred, dim = 1))
    # print("train")
    # print(f"acutal: {y_train}")
    # print(f"pred: {torch.argmax(y_pred, dim = 1)}")
    train_loss += loss
    train_acc += acc

    # 3. optimizer zero grad
    optimizer.zero_grad()

    # 4. Backward
    loss.backward()

    # 5. optimizer step
    optimizer.step()

  train_loss /= len(dataloader)
  train_acc /= len(dataloader)

  return train_loss, train_acc


def test_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module, accuracy_fn, device: torch.device = device):
  test_loss, test_acc = 0, 0

  model.eval()
  with torch.inference_mode():
    for x_test, y_test in dataloader:
      if device == 'cuda':
        x_test, y_test = x_test.to(device), y_test.to(device)

      # 1. Forward
      test_pred = model(x_test)

      # 2. Loss
      test_loss += loss_fn(test_pred, y_test)
      test_acc += accuracy_fn(y_test, torch.argmax(test_pred, dim = 1))
      # print("test")
      # print(f"acutal: {y_test}")
      # print(f"pred: {torch.argmax(test_pred, dim = 1)}")

    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

  return test_loss, test_acc

"""## Loss and Accuracy plot vs Epoch"""

def plotplot(train_losses, test_losses, train_acces, test_acces):
  plt.figure(figsize = (25,8))
  # plt.plot(range(1, epoches*(len(train_dataloader)*BATCH_SIZE) + 1), train)
  plt.subplot(1,2,1)
  plt.plot(range(len(train_losses)),train_losses, label = "Train Loss")
  plt.plot(range(len(test_losses)),test_losses, label = "Test Loss")
  plt.xlabel("Epoches")
  plt.ylabel("Loss")
  plt.title("Loss vs Epoches")
  plt.legend()

  plt.subplot(1,2,2)
  plt.plot(range(len(train_acces)),train_acces, label = "Train Accuracy")
  plt.plot(range(len(test_acces)),test_acces, label = "Test Accuracy")
  plt.xlabel("Epoches")
  plt.ylabel("Accuracy")
  plt.title("Accuracy vs Epoches")
  plt.legend()

  plt.show()

"""## only train teacher"""

torch.manual_seed(64)
torch.cuda.manual_seed(64)
teacher = TeacherCNN(in_channel = 3, out_channel = 12, hidden_channel = 10, in_feature = 512, out_feature = num_classes).to(device)

# initialize Xavier weights
teacher.apply(weights_init)

# optimizer function
optimizer = torch.optim.Adam(params = teacher.parameters(), lr = 1e-3)

train_losses, test_losses = [], []
train_acces, test_acces = [], []

# train model
epoches = 10

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  train_loss, train_acc = train_loop(model = teacher, dataloader = train_dataloader,
                                     loss_fn = loss_fn, optimizer = optimizer, 
                                     accuracy_fn = accuracy_fn, device = device)
  test_loss, test_acc = test_loop(model = teacher, dataloader = valid_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  print(f"Epoch: {epoch + 1}  Train Loss: {train_loss:.4f} / Test Loss: {test_loss:.4f} -/- Train Accuracy: {train_acc:.4f} / Test Accuracy: {test_acc:.4f}")

  train_losses.append(train_loss.item())
  test_losses.append(test_loss.item())
  train_acces.append(train_acc.item())
  test_acces.append(test_acc.item())

plotplot(train_losses, test_losses, train_acces, test_acces)

"""## Train student network"""

torch.manual_seed(64)
torch.cuda.manual_seed(64)
student = StudentCNN(in_channel = 3, out_channel = 12, hidden_channel = 10, in_feature = 512, out_feature = num_classes).to(device)

# initialize Xavier weights
student.apply(weights_init)

# optimizer function
optimizer = torch.optim.Adam(params = teacher.parameters(), lr = 1e-4)


train_losses, test_losses = [], []
train_acces, test_acces = [], []

# train model
epoches = 10

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  train_loss, train_acc = train_loop(model = student, dataloader = train_dataloader,
                                     loss_fn = loss_fn, optimizer = optimizer, 
                                     accuracy_fn = accuracy_fn, device = device)
  test_loss, test_acc = test_loop(model = student, dataloader = valid_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  print(f"Epoch: {epoch + 1}  Train Loss: {train_loss:.4f} / Test Loss: {test_loss:.4f} -/- Train Accuracy: {train_acc:.4f} / Test Accuracy: {test_acc:.4f}")

  train_losses.append(train_loss.item())
  test_losses.append(test_loss.item())
  train_acces.append(train_acc.item())
  test_acces.append(test_acc.item())

plotplot(train_losses, test_losses, train_acces, test_acces)

"""## EMA"""

class EMA():
  def cal_loss_with(student_loss, teacher_loss, alpha):
    loss = alpha * student_loss + (1-alpha) * teacher_loss
    return loss

  def cal_loss_without(student_pred, teacher_pred, loss_fn):
    loss = loss_fn(student_pred, teacher_pred)
    return loss

ema = EMA()

"""## train loop for TS with EMA"""

# train
def st_train_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, 
               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, 
               accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0

  alpha = 0.2
  for batch, (x_train, y_train) in enumerate(dataloader):

    if device == 'cuda':
      x_train, y_train = x_train.to(device), y_train.to(device)

    model.train()
    # 1. Forward
    y_train_teacher = teacher(x_train)
    y_pred = model(x_train)

    # 2. Loss
    st_loss = loss_fn(y_pred, y_train)
    ta_loss = loss_fn(y_train_teacher, y_pred)
    loss = ema.cal_loss_with(st_loss, ta_loss, alpha)
    acc = accuracy_fn(y_train, torch.argmax(y_pred, dim = 1))
    train_loss += loss
    train_acc += acc

    # 3. optimizer zero grad
    optimizer.zero_grad()

    # 4. Backward
    loss.backward()

    # 5. optimizer step
    optimizer.step()

  train_loss /= len(dataloader)
  train_acc /= len(dataloader)

  return train_loss, train_acc


def st_test_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module, accuracy_fn, device: torch.device = device):
  test_loss, test_acc = 0, 0

  model.eval()
  with torch.inference_mode():
    for x_test, y_test in dataloader:
      if device == 'cuda':
        x_test, y_test = x_test.to(device), y_test.to(device)

      # 1. Forward
      test_pred = model(x_test)

      # 2. Loss
      test_loss += loss_fn(test_pred, y_test)
      test_acc += accuracy_fn(y_test, torch.argmax(test_pred, dim = 1))

    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

  return test_loss, test_acc

""" ## Train student from teacher"""

torch.manual_seed(64)
torch.cuda.manual_seed(64)
student = StudentCNN(in_channel = 3, out_channel = 12, hidden_channel = 10, in_feature = 512, out_feature = num_classes).to(device)

# initialize Xavier weights
student.apply(weights_init)

# optimizer function
optimizer = torch.optim.Adam(params = teacher.parameters(), lr = 1e-4)


train_losses, test_losses = [], []
train_acces, test_acces = [], []

# train model
epoches = 10

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  train_loss, train_acc = st_train_loop(model = student, dataloader = train_dataloader,
                                     loss_fn = loss_fn, optimizer = optimizer, 
                                     accuracy_fn = accuracy_fn, device = device)
  test_loss, test_acc = st_test_loop(model = student, dataloader = valid_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  print(f"Epoch: {epoch + 1}  Train Loss: {train_loss:.4f} / Test Loss: {test_loss:.4f} -/- Train Accuracy: {train_acc:.4f} / Test Accuracy: {test_acc:.4f}")

  train_losses.append(train_loss.item())
  test_losses.append(test_loss.item())
  train_acces.append(train_acc.item())
  test_acces.append(test_acc.item())

plotplot(train_losses, test_losses, train_acces, test_acces)

"""## train loop for TS without EMA"""

# train
def NOEMA_train_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, 
               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, 
               accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0

  alpha = 0.2
  for batch, (x_train, y_train) in enumerate(dataloader):

    if device == 'cuda':
      x_train, y_train = x_train.to(device), y_train.to(device)

    model.train()
    # 1. Forward
    y_train_teacher = teacher(x_train)
    y_pred = model(x_train)

    # 2. Loss
    loss = cal_loss_without(y_pred, y_train_teacher, loss_fn)
    # ta_loss = loss_fn(y_train_teacher, y_pred)

    acc = accuracy_fn(y_train, torch.argmax(y_pred, dim = 1))
    train_loss += loss
    train_acc += acc

    # 3. optimizer zero grad
    optimizer.zero_grad()

    # 4. Backward
    loss.backward()

    # 5. optimizer step
    optimizer.step()

  train_loss /= len(dataloader)
  train_acc /= len(dataloader)

  return train_loss, train_acc


def NOEMA_test_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module, accuracy_fn, device: torch.device = device):
  test_loss, test_acc = 0, 0

  model.eval()
  with torch.inference_mode():
    for x_test, y_test in dataloader:
      if device == 'cuda':
        x_test, y_test = x_test.to(device), y_test.to(device)

      # 1. Forward
      test_pred = model(x_test)

      # 2. Loss
      test_loss += loss_fn(test_pred, y_test)
      test_acc += accuracy_fn(y_test, torch.argmax(test_pred, dim = 1))

    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

  return test_loss, test_acc

""" ## Train student from teacher"""

torch.manual_seed(64)
torch.cuda.manual_seed(64)
student = StudentCNN(in_channel = 3, out_channel = 12, hidden_channel = 10, in_feature = 512, out_feature = num_classes).to(device)

# initialize Xavier weights
student.apply(weights_init)

# optimizer function
optimizer = torch.optim.Adam(params = teacher.parameters(), lr = 1e-4)


train_losses, test_losses = [], []
train_acces, test_acces = [], []

# train model
epoches = 10

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  train_loss, train_acc = NOEMA_train_loop(model = student, dataloader = train_dataloader,
                                     loss_fn = loss_fn, optimizer = optimizer, 
                                     accuracy_fn = accuracy_fn, device = device)
  test_loss, test_acc = NOEMA_test_loop(model = student, dataloader = valid_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  print(f"Epoch: {epoch + 1}  Train Loss: {train_loss:.4f} / Test Loss: {test_loss:.4f} -/- Train Accuracy: {train_acc:.4f} / Test Accuracy: {test_acc:.4f}")

  train_losses.append(train_loss.item())
  test_losses.append(test_loss.item())
  train_acces.append(train_acc.item())
  test_acces.append(test_acc.item())

plotplot(train_losses, test_losses, train_acces, test_acces)

